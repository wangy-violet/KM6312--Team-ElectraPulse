{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:29.221732Z",
     "iopub.status.busy": "2024-11-22T16:19:29.220914Z",
     "iopub.status.idle": "2024-11-22T16:19:29.366003Z",
     "shell.execute_reply": "2024-11-22T16:19:29.365069Z",
     "shell.execute_reply.started": "2024-11-22T16:19:29.221694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:30.786216Z",
     "iopub.status.busy": "2024-11-22T16:19:30.785829Z",
     "iopub.status.idle": "2024-11-22T16:19:32.748593Z",
     "shell.execute_reply": "2024-11-22T16:19:32.747492Z",
     "shell.execute_reply.started": "2024-11-22T16:19:30.786183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "csv_file_path = '/kaggle/input/dataset/combined_tweets.csv'\n",
    "\n",
    "# 读取CSV文件\n",
    "combined_tweets = pd.read_csv(csv_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:32.750576Z",
     "iopub.status.busy": "2024-11-22T16:19:32.750241Z",
     "iopub.status.idle": "2024-11-22T16:19:33.470658Z",
     "shell.execute_reply": "2024-11-22T16:19:33.469223Z",
     "shell.execute_reply.started": "2024-11-22T16:19:32.750545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 提取特征和标签\n",
    "X = combined_tweets['lemma']  # 词形还原后的推文\n",
    "y = combined_tweets['sentiment']  # 情感标签：积极、消极、中立\n",
    "\n",
    "X = X.astype(str).fillna(\"\")\n",
    "\n",
    "# 转换文本数据为TF-IDF特征矩阵\n",
    "vectorizer = TfidfVectorizer(max_features=5000)  # 选择最多5000个特征\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# 将情感标签编码为数值\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)  # 例如：positive -> 0, negative -> 1, neutral -> 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:35.209542Z",
     "iopub.status.busy": "2024-11-22T16:19:35.209192Z",
     "iopub.status.idle": "2024-11-22T16:19:35.223980Z",
     "shell.execute_reply": "2024-11-22T16:19:35.222735Z",
     "shell.execute_reply.started": "2024-11-22T16:19:35.209513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y_encoded, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:55:49.344100Z",
     "iopub.status.busy": "2024-11-19T21:55:49.343641Z",
     "iopub.status.idle": "2024-11-19T21:56:04.682908Z",
     "shell.execute_reply": "2024-11-19T21:56:04.681018Z",
     "shell.execute_reply.started": "2024-11-19T21:55:49.344047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8676955702167767\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.79      0.83      2292\n",
      "     Neutral       0.80      0.89      0.84      2548\n",
      "    Positive       0.91      0.91      0.91      3648\n",
      "\n",
      "    accuracy                           0.87      8488\n",
      "   macro avg       0.87      0.86      0.86      8488\n",
      "weighted avg       0.87      0.87      0.87      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义模型\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 训练模型\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:56:04.685210Z",
     "iopub.status.busy": "2024-11-19T21:56:04.684623Z",
     "iopub.status.idle": "2024-11-19T21:56:16.473707Z",
     "shell.execute_reply": "2024-11-19T21:56:16.472581Z",
     "shell.execute_reply.started": "2024-11-19T21:56:04.685147Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7196041470311028\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.61      0.63      2292\n",
      "     Neutral       0.68      0.76      0.72      2548\n",
      "    Positive       0.78      0.76      0.77      3648\n",
      "\n",
      "    accuracy                           0.72      8488\n",
      "   macro avg       0.71      0.71      0.71      8488\n",
      "weighted avg       0.72      0.72      0.72      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 决策树模型\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred_tree = decision_tree.predict(X_test)\n",
    "\n",
    "# 决策树结果\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_tree))\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_tree, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:56:16.475759Z",
     "iopub.status.busy": "2024-11-19T21:56:16.475306Z",
     "iopub.status.idle": "2024-11-19T21:56:16.522277Z",
     "shell.execute_reply": "2024-11-19T21:56:16.521081Z",
     "shell.execute_reply.started": "2024-11-19T21:56:16.475711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 0.7455230914231856\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.82      0.67      0.74      2292\n",
      "     Neutral       0.76      0.59      0.67      2548\n",
      "    Positive       0.71      0.90      0.79      3648\n",
      "\n",
      "    accuracy                           0.75      8488\n",
      "   macro avg       0.76      0.72      0.73      8488\n",
      "weighted avg       0.75      0.75      0.74      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义模型\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "# 训练模型\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"Naive Bayes Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:56:16.524060Z",
     "iopub.status.busy": "2024-11-19T21:56:16.523696Z",
     "iopub.status.idle": "2024-11-19T21:57:05.596498Z",
     "shell.execute_reply": "2024-11-19T21:57:05.595340Z",
     "shell.execute_reply.started": "2024-11-19T21:56:16.524029Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8109095193213949\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.84      0.68      0.75      2292\n",
      "     Neutral       0.74      0.87      0.80      2548\n",
      "    Positive       0.86      0.85      0.85      3648\n",
      "\n",
      "    accuracy                           0.81      8488\n",
      "   macro avg       0.81      0.80      0.80      8488\n",
      "weighted avg       0.82      0.81      0.81      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义模型\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:57:05.598245Z",
     "iopub.status.busy": "2024-11-19T21:57:05.597890Z",
     "iopub.status.idle": "2024-11-19T21:57:39.939674Z",
     "shell.execute_reply": "2024-11-19T21:57:39.938557Z",
     "shell.execute_reply.started": "2024-11-19T21:57:05.598214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8036050895381716\n",
      "XGBoost Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.67      0.76      2292\n",
      "     Neutral       0.71      0.88      0.78      2548\n",
      "    Positive       0.86      0.84      0.85      3648\n",
      "\n",
      "    accuracy                           0.80      8488\n",
      "   macro avg       0.81      0.79      0.80      8488\n",
      "weighted avg       0.82      0.80      0.80      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义模型\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"XGBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T21:57:39.941626Z",
     "iopub.status.busy": "2024-11-19T21:57:39.941216Z",
     "iopub.status.idle": "2024-11-19T22:06:25.831269Z",
     "shell.execute_reply": "2024-11-19T22:06:25.827012Z",
     "shell.execute_reply.started": "2024-11-19T21:57:39.941591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Classifier Accuracy: 0.8779453345900095\n",
      "Stacking Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.83      0.85      2292\n",
      "     Neutral       0.83      0.88      0.85      2548\n",
      "    Positive       0.92      0.91      0.91      3648\n",
      "\n",
      "    accuracy                           0.88      8488\n",
      "   macro avg       0.87      0.87      0.87      8488\n",
      "weighted avg       0.88      0.88      0.88      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义基分类器\n",
    "base_estimators = [\n",
    "    ('logreg', LogisticRegression(max_iter=1000)),\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42)),  # 替换为决策树\n",
    "    ('naive_bayes', MultinomialNB()),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgboost', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "]\n",
    "\n",
    "# 定义元分类器（逻辑回归）\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(max_iter=1000),\n",
    "    cv=5  # 交叉验证\n",
    ")\n",
    "\n",
    "# 训练 Stacking 模型\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_stacking = stacking_model.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"Stacking Classifier Accuracy:\", accuracy_score(y_test, y_pred_stacking))\n",
    "print(\"Stacking Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_stacking, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble learning stacking - voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T22:06:25.836988Z",
     "iopub.status.busy": "2024-11-19T22:06:25.836218Z",
     "iopub.status.idle": "2024-11-19T22:08:13.455747Z",
     "shell.execute_reply": "2024-11-19T22:08:13.454178Z",
     "shell.execute_reply.started": "2024-11-19T22:06:25.836890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Accuracy: 0.8210414703110274\n",
      "Voting Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.71      0.77      2292\n",
      "     Neutral       0.77      0.85      0.81      2548\n",
      "    Positive       0.86      0.87      0.86      3648\n",
      "\n",
      "    accuracy                           0.82      8488\n",
      "   macro avg       0.82      0.81      0.81      8488\n",
      "weighted avg       0.82      0.82      0.82      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 定义基分类器\n",
    "voting_estimators = [\n",
    "    ('logreg', LogisticRegression(max_iter=1000)),\n",
    "    ('decision_tree', DecisionTreeClassifier(random_state=42)),  # 替换为决策树\n",
    "    ('naive_bayes', MultinomialNB()),\n",
    "    ('random_forest', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgboost', XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))\n",
    "]\n",
    "\n",
    "# 定义 Voting 模型（软投票）\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=voting_estimators,\n",
    "    voting='soft'  # 使用软投票 ('hard' 表示硬投票)\n",
    ")\n",
    "\n",
    "# 训练 Voting 模型\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# 预测\n",
    "y_pred_voting = voting_model.predict(X_test)\n",
    "\n",
    "# 输出评估指标\n",
    "print(\"Voting Classifier Accuracy:\", accuracy_score(y_test, y_pred_voting))\n",
    "print(\"Voting Classifier Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_voting, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:49.438944Z",
     "iopub.status.busy": "2024-11-22T16:19:49.438530Z",
     "iopub.status.idle": "2024-11-22T16:19:49.445322Z",
     "shell.execute_reply": "2024-11-22T16:19:49.444161Z",
     "shell.execute_reply.started": "2024-11-22T16:19:49.438908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 将情感标签转为 One-Hot 编码\n",
    "y_train_onehot = to_categorical(y_train)\n",
    "y_test_onehot = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:19:52.732698Z",
     "iopub.status.busy": "2024-11-22T16:19:52.732313Z",
     "iopub.status.idle": "2024-11-22T16:20:11.908155Z",
     "shell.execute_reply": "2024-11-22T16:20:11.907155Z",
     "shell.execute_reply.started": "2024-11-22T16:19:52.732662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.4388 - loss: 1.0465 - val_accuracy: 0.5622 - val_loss: 0.9429\n",
      "Epoch 2/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6205 - loss: 0.9051 - val_accuracy: 0.7049 - val_loss: 0.8497\n",
      "Epoch 3/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.8101 - val_accuracy: 0.7486 - val_loss: 0.7806\n",
      "Epoch 4/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7932 - loss: 0.7364 - val_accuracy: 0.7779 - val_loss: 0.7271\n",
      "Epoch 5/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8235 - loss: 0.6830 - val_accuracy: 0.7936 - val_loss: 0.6841\n",
      "Epoch 6/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.6339 - val_accuracy: 0.8121 - val_loss: 0.6491\n",
      "Epoch 7/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8538 - loss: 0.5950 - val_accuracy: 0.8212 - val_loss: 0.6200\n",
      "Epoch 8/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8635 - loss: 0.5583 - val_accuracy: 0.8295 - val_loss: 0.5952\n",
      "Epoch 9/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8702 - loss: 0.5332 - val_accuracy: 0.8365 - val_loss: 0.5742\n",
      "Epoch 10/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8816 - loss: 0.5080 - val_accuracy: 0.8420 - val_loss: 0.5564\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Basic Neural Network Accuracy: 0.8420122525918945\n",
      "Basic Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.76      0.81      2292\n",
      "     Neutral       0.78      0.84      0.81      2548\n",
      "    Positive       0.87      0.90      0.88      3648\n",
      "\n",
      "    accuracy                           0.84      8488\n",
      "   macro avg       0.84      0.83      0.83      8488\n",
      "weighted avg       0.84      0.84      0.84      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义最简单的神经网络模型\n",
    "basic_nn_model = Sequential([\n",
    "    Dense(3, activation='softmax', input_shape=(X_train.shape[1],))  # 输入层和输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "basic_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "basic_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = basic_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Basic Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Basic Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T22:08:52.055838Z",
     "iopub.status.busy": "2024-11-19T22:08:52.055455Z",
     "iopub.status.idle": "2024-11-19T22:09:53.180290Z",
     "shell.execute_reply": "2024-11-19T22:09:53.179055Z",
     "shell.execute_reply.started": "2024-11-19T22:08:52.055794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.6466 - loss: 0.7785 - val_accuracy: 0.8804 - val_loss: 0.3778\n",
      "Epoch 2/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9146 - loss: 0.2833 - val_accuracy: 0.8906 - val_loss: 0.3451\n",
      "Epoch 3/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9353 - loss: 0.2104 - val_accuracy: 0.8911 - val_loss: 0.3434\n",
      "Epoch 4/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9497 - loss: 0.1602 - val_accuracy: 0.8909 - val_loss: 0.3539\n",
      "Epoch 5/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9617 - loss: 0.1301 - val_accuracy: 0.8856 - val_loss: 0.3888\n",
      "Epoch 6/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9719 - loss: 0.0968 - val_accuracy: 0.8812 - val_loss: 0.4270\n",
      "Epoch 7/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9810 - loss: 0.0709 - val_accuracy: 0.8803 - val_loss: 0.4720\n",
      "Epoch 8/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.0543 - val_accuracy: 0.8803 - val_loss: 0.5232\n",
      "Epoch 9/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9914 - loss: 0.0393 - val_accuracy: 0.8799 - val_loss: 0.5670\n",
      "Epoch 10/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9939 - loss: 0.0297 - val_accuracy: 0.8778 - val_loss: 0.6089\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Hidden Neural Network Accuracy: 0.8778275212064091\n",
      "Hidden Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.85      0.86      2292\n",
      "     Neutral       0.85      0.85      0.85      2548\n",
      "    Positive       0.91      0.91      0.91      3648\n",
      "\n",
      "    accuracy                           0.88      8488\n",
      "   macro avg       0.87      0.87      0.87      8488\n",
      "weighted avg       0.88      0.88      0.88      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型：输入层 + 隐藏层 + 输出层\n",
    "hidden_nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 隐藏层\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "hidden_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "hidden_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = hidden_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Hidden Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Hidden Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Hidden + early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:32:53.236824Z",
     "iopub.status.busy": "2024-11-22T16:32:53.236356Z",
     "iopub.status.idle": "2024-11-22T16:33:21.474842Z",
     "shell.execute_reply": "2024-11-22T16:33:21.473773Z",
     "shell.execute_reply.started": "2024-11-22T16:32:53.236785Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6465 - loss: 0.7819 - val_accuracy: 0.8787 - val_loss: 0.3842\n",
      "Epoch 2/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9121 - loss: 0.2888 - val_accuracy: 0.8911 - val_loss: 0.3458\n",
      "Epoch 3/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9358 - loss: 0.2123 - val_accuracy: 0.8880 - val_loss: 0.3482\n",
      "Epoch 4/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1650 - val_accuracy: 0.8888 - val_loss: 0.3631\n",
      "Epoch 5/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9614 - loss: 0.1281 - val_accuracy: 0.8856 - val_loss: 0.3904\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "EarlyStopping Model Accuracy: 0.8911404335532517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.86      0.87      2292\n",
      "     Neutral       0.87      0.87      0.87      2548\n",
      "    Positive       0.91      0.93      0.92      3648\n",
      "\n",
      "    accuracy                           0.89      8488\n",
      "   macro avg       0.89      0.88      0.89      8488\n",
      "weighted avg       0.89      0.89      0.89      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型\n",
    "earlystopping_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 隐藏层\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "earlystopping_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 设置 EarlyStopping 回调\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # 监控验证集损失\n",
    "    patience=3,  # 容忍验证损失不改善的轮数\n",
    "    restore_best_weights=True  # 恢复验证集表现最好的权重\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "earlystopping_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=50,  # 设置较大的最大训练轮数\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 获取预测结果和评估\n",
    "y_pred_probs = earlystopping_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test_onehot, axis=1)\n",
    "\n",
    "print(\"EarlyStopping Model Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Hidden Layer + Early stop + LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:35:00.968038Z",
     "iopub.status.busy": "2024-11-22T16:35:00.967586Z",
     "iopub.status.idle": "2024-11-22T16:35:33.498063Z",
     "shell.execute_reply": "2024-11-22T16:35:33.496949Z",
     "shell.execute_reply.started": "2024-11-22T16:35:00.968002Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6472 - loss: 0.7812 - val_accuracy: 0.8738 - val_loss: 0.3906 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9140 - loss: 0.2849 - val_accuracy: 0.8884 - val_loss: 0.3498 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9361 - loss: 0.2129 - val_accuracy: 0.8882 - val_loss: 0.3469 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9498 - loss: 0.1637 - val_accuracy: 0.8869 - val_loss: 0.3603 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9601 - loss: 0.1317 - val_accuracy: 0.8834 - val_loss: 0.3902 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9741 - loss: 0.0926 - val_accuracy: 0.8874 - val_loss: 0.4011 - learning_rate: 5.0000e-04\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "LearningRateScheduler Model Accuracy: 0.8881950989632422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.87      0.86      0.87      2292\n",
      "     Neutral       0.87      0.87      0.87      2548\n",
      "    Positive       0.91      0.92      0.92      3648\n",
      "\n",
      "    accuracy                           0.89      8488\n",
      "   macro avg       0.88      0.88      0.88      8488\n",
      "weighted avg       0.89      0.89      0.89      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# 定义学习率调度函数\n",
    "def lr_schedule(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.5  # 每 5 个 epoch 学习率减半\n",
    "\n",
    "# 定义神经网络模型\n",
    "lrscheduler_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 隐藏层\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "lrscheduler_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 添加 EarlyStopping 和 LearningRateScheduler 回调\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# 训练模型\n",
    "lrscheduler_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# 评估性能\n",
    "y_pred_probs = lrscheduler_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"LearningRateScheduler Model Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Hidden Layer + Early stop + LearningRateScheduler + BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:35:57.875652Z",
     "iopub.status.busy": "2024-11-22T16:35:57.875275Z",
     "iopub.status.idle": "2024-11-22T16:36:26.854406Z",
     "shell.execute_reply": "2024-11-22T16:36:26.853249Z",
     "shell.execute_reply.started": "2024-11-22T16:35:57.875619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.6940 - loss: 0.6952 - val_accuracy: 0.8612 - val_loss: 0.3984 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9222 - loss: 0.2354 - val_accuracy: 0.8679 - val_loss: 0.3794 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9589 - loss: 0.1268 - val_accuracy: 0.8682 - val_loss: 0.4508 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9791 - loss: 0.0724 - val_accuracy: 0.8612 - val_loss: 0.5277 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9850 - loss: 0.0502 - val_accuracy: 0.8605 - val_loss: 0.6028 - learning_rate: 0.0010\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "BatchNormalization Model Accuracy: 0.8679311969839774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.86      0.85      2292\n",
      "     Neutral       0.87      0.81      0.84      2548\n",
      "    Positive       0.89      0.91      0.90      3648\n",
      "\n",
      "    accuracy                           0.87      8488\n",
      "   macro avg       0.86      0.86      0.86      8488\n",
      "weighted avg       0.87      0.87      0.87      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# 定义神经网络模型\n",
    "batchnorm_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),  # 批量归一化层\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "batchnorm_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 添加 EarlyStopping 和 LearningRateScheduler 回调\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "# 训练模型\n",
    "batchnorm_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# 评估性能\n",
    "y_pred_probs = batchnorm_model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "print(\"BatchNormalization Model Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + multiple hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T15:44:32.615111Z",
     "iopub.status.busy": "2024-11-22T15:44:32.614731Z",
     "iopub.status.idle": "2024-11-22T15:46:25.900661Z",
     "shell.execute_reply": "2024-11-22T15:46:25.899416Z",
     "shell.execute_reply.started": "2024-11-22T15:44:32.615079Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.6765 - loss: 0.7077 - val_accuracy: 0.8799 - val_loss: 0.3490\n",
      "Epoch 2/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9176 - loss: 0.2376 - val_accuracy: 0.8837 - val_loss: 0.3292\n",
      "Epoch 3/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9531 - loss: 0.1441 - val_accuracy: 0.8855 - val_loss: 0.3534\n",
      "Epoch 4/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9752 - loss: 0.0799 - val_accuracy: 0.8817 - val_loss: 0.4449\n",
      "Epoch 5/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9893 - loss: 0.0347 - val_accuracy: 0.8843 - val_loss: 0.5403\n",
      "Epoch 6/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9957 - loss: 0.0152 - val_accuracy: 0.8752 - val_loss: 0.6477\n",
      "Epoch 7/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9970 - loss: 0.0108 - val_accuracy: 0.8763 - val_loss: 0.7253\n",
      "Epoch 8/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9983 - loss: 0.0072 - val_accuracy: 0.8810 - val_loss: 0.7466\n",
      "Epoch 9/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9983 - loss: 0.0064 - val_accuracy: 0.8764 - val_loss: 0.8276\n",
      "Epoch 10/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9979 - loss: 0.0078 - val_accuracy: 0.8787 - val_loss: 0.8387\n",
      "Epoch 11/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9977 - loss: 0.0073 - val_accuracy: 0.8731 - val_loss: 0.8644\n",
      "Epoch 12/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9983 - loss: 0.0060 - val_accuracy: 0.8770 - val_loss: 0.8758\n",
      "Epoch 13/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9989 - loss: 0.0049 - val_accuracy: 0.8709 - val_loss: 0.8581\n",
      "Epoch 14/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9994 - loss: 0.0032 - val_accuracy: 0.8748 - val_loss: 0.9220\n",
      "Epoch 15/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9992 - loss: 0.0028 - val_accuracy: 0.8748 - val_loss: 0.9680\n",
      "Epoch 16/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9979 - loss: 0.0064 - val_accuracy: 0.8710 - val_loss: 0.9740\n",
      "Epoch 17/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0058 - val_accuracy: 0.8788 - val_loss: 0.9630\n",
      "Epoch 18/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9989 - loss: 0.0031 - val_accuracy: 0.8726 - val_loss: 0.8988\n",
      "Epoch 19/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9993 - loss: 0.0030 - val_accuracy: 0.8772 - val_loss: 1.0089\n",
      "Epoch 20/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9991 - loss: 0.0039 - val_accuracy: 0.8746 - val_loss: 0.9550\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Hidden Neural Network Accuracy: 0.8746465598491989\n",
      "Hidden Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.81      0.84      2292\n",
      "     Neutral       0.82      0.87      0.84      2548\n",
      "    Positive       0.91      0.92      0.91      3648\n",
      "\n",
      "    accuracy                           0.87      8488\n",
      "   macro avg       0.87      0.87      0.87      8488\n",
      "weighted avg       0.88      0.87      0.87      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型：多隐藏层 + 输出层\n",
    "hidden_nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 第1隐藏层\n",
    "    Dense(64, activation='relu'),  # 第2隐藏层\n",
    "    Dense(32, activation='relu'),  # 第3隐藏层\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "hidden_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "hidden_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=20,  # 增加训练轮数\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = hidden_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Hidden Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Hidden Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Hidden layer + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T22:09:53.182029Z",
     "iopub.status.busy": "2024-11-19T22:09:53.181651Z",
     "iopub.status.idle": "2024-11-19T22:10:57.093199Z",
     "shell.execute_reply": "2024-11-19T22:10:57.091821Z",
     "shell.execute_reply.started": "2024-11-19T22:09:53.181995Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.6102 - loss: 0.8451 - val_accuracy: 0.8716 - val_loss: 0.4155\n",
      "Epoch 2/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8928 - loss: 0.3507 - val_accuracy: 0.8914 - val_loss: 0.3474\n",
      "Epoch 3/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9239 - loss: 0.2499 - val_accuracy: 0.8951 - val_loss: 0.3301\n",
      "Epoch 4/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9368 - loss: 0.2078 - val_accuracy: 0.8992 - val_loss: 0.3269\n",
      "Epoch 5/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9500 - loss: 0.1633 - val_accuracy: 0.8975 - val_loss: 0.3310\n",
      "Epoch 6/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1430 - val_accuracy: 0.8976 - val_loss: 0.3404\n",
      "Epoch 7/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9644 - loss: 0.1190 - val_accuracy: 0.8980 - val_loss: 0.3570\n",
      "Epoch 8/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9697 - loss: 0.1024 - val_accuracy: 0.8931 - val_loss: 0.3738\n",
      "Epoch 9/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9749 - loss: 0.0883 - val_accuracy: 0.8915 - val_loss: 0.3906\n",
      "Epoch 10/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0765 - val_accuracy: 0.8947 - val_loss: 0.4045\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Dropout Neural Network Accuracy: 0.894674835061263\n",
      "Dropout Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.88      0.88      0.88      2292\n",
      "     Neutral       0.87      0.86      0.86      2548\n",
      "    Positive       0.92      0.93      0.92      3648\n",
      "\n",
      "    accuracy                           0.89      8488\n",
      "   macro avg       0.89      0.89      0.89      8488\n",
      "weighted avg       0.89      0.89      0.89      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型：隐藏层 + Dropout + 输出层\n",
    "dropout_nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 隐藏层\n",
    "    Dropout(0.5),  # Dropout 层，设置 50% 的节点随机失活\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "dropout_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "dropout_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = dropout_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Dropout Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Dropout Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + multiple Hidden layer + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T22:10:57.095810Z",
     "iopub.status.busy": "2024-11-19T22:10:57.095222Z",
     "iopub.status.idle": "2024-11-19T22:12:48.580516Z",
     "shell.execute_reply": "2024-11-19T22:12:48.579326Z",
     "shell.execute_reply.started": "2024-11-19T22:10:57.095754Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 11ms/step - accuracy: 0.6506 - loss: 0.7531 - val_accuracy: 0.8679 - val_loss: 0.3793\n",
      "Epoch 2/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9108 - loss: 0.2673 - val_accuracy: 0.8858 - val_loss: 0.3354\n",
      "Epoch 3/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9466 - loss: 0.1587 - val_accuracy: 0.8845 - val_loss: 0.3596\n",
      "Epoch 4/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9654 - loss: 0.1060 - val_accuracy: 0.8823 - val_loss: 0.4166\n",
      "Epoch 5/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9773 - loss: 0.0703 - val_accuracy: 0.8811 - val_loss: 0.4816\n",
      "Epoch 6/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9837 - loss: 0.0474 - val_accuracy: 0.8809 - val_loss: 0.5412\n",
      "Epoch 7/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9895 - loss: 0.0336 - val_accuracy: 0.8808 - val_loss: 0.5967\n",
      "Epoch 8/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9926 - loss: 0.0255 - val_accuracy: 0.8761 - val_loss: 0.6076\n",
      "Epoch 9/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 9ms/step - accuracy: 0.9921 - loss: 0.0284 - val_accuracy: 0.8807 - val_loss: 0.6403\n",
      "Epoch 10/10\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 10ms/step - accuracy: 0.9936 - loss: 0.0204 - val_accuracy: 0.8830 - val_loss: 0.6859\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Multi-Hidden Layer Neural Network Accuracy: 0.8830113100848256\n",
      "Multi-Hidden Layer Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.86      0.87      0.87      2292\n",
      "     Neutral       0.86      0.84      0.85      2548\n",
      "    Positive       0.91      0.92      0.92      3648\n",
      "\n",
      "    accuracy                           0.88      8488\n",
      "   macro avg       0.88      0.88      0.88      8488\n",
      "weighted avg       0.88      0.88      0.88      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型：多层隐藏层 + Dropout\n",
    "multi_hidden_nn_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # 第一隐藏层\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),  # 第二隐藏层\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),  # 第三隐藏层\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "multi_hidden_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "multi_hidden_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = multi_hidden_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Multi-Hidden Layer Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Multi-Hidden Layer Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + Multiple Hidden layers + Dropout + Early stop + BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T22:12:48.582355Z",
     "iopub.status.busy": "2024-11-19T22:12:48.581972Z",
     "iopub.status.idle": "2024-11-19T22:14:03.922590Z",
     "shell.execute_reply": "2024-11-19T22:14:03.921328Z",
     "shell.execute_reply.started": "2024-11-19T22:12:48.582321Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 1/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 12ms/step - accuracy: 0.5653 - loss: 1.0008 - val_accuracy: 0.8464 - val_loss: 0.4289 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 2/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.8623 - loss: 0.3848 - val_accuracy: 0.8627 - val_loss: 0.3761 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 3/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9038 - loss: 0.2748 - val_accuracy: 0.8718 - val_loss: 0.3706 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 4/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9219 - loss: 0.2173 - val_accuracy: 0.8653 - val_loss: 0.4044 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0010000000474974513.\n",
      "Epoch 5/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 11ms/step - accuracy: 0.9416 - loss: 0.1718 - val_accuracy: 0.8666 - val_loss: 0.4422 - learning_rate: 0.0010\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0005000000237487257.\n",
      "Epoch 6/20\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11ms/step - accuracy: 0.9562 - loss: 0.1257 - val_accuracy: 0.8750 - val_loss: 0.4636 - learning_rate: 5.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 3.\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Multi-Hidden Layer Neural Network Accuracy: 0.8718190386427899\n",
      "Multi-Hidden Layer Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.85      0.86      0.85      2292\n",
      "     Neutral       0.86      0.82      0.84      2548\n",
      "    Positive       0.89      0.92      0.90      3648\n",
      "\n",
      "    accuracy                           0.87      8488\n",
      "   macro avg       0.87      0.86      0.87      8488\n",
      "weighted avg       0.87      0.87      0.87      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义学习率调度函数\n",
    "def lr_scheduler(epoch, lr):\n",
    "    \"\"\"根据epoch动态调整学习率.\"\"\"\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * 0.5\n",
    "\n",
    "# 定义回调函数\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True, \n",
    "    verbose=1\n",
    ")\n",
    "lr_scheduler_callback = LearningRateScheduler(lr_scheduler, verbose=1)\n",
    "\n",
    "# 定义神经网络模型：添加 BatchNormalization\n",
    "multi_hidden_nn_model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),  # 第一隐藏层\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),  # 第二隐藏层\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),  # 第三隐藏层\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "multi_hidden_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 训练模型，添加回调函数\n",
    "multi_hidden_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=20,  # 增加足够的epochs以适应EarlyStopping\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, lr_scheduler_callback]  # 添加回调\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = multi_hidden_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"Multi-Hidden Layer Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"Multi-Hidden Layer Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras + multiple layers + batch normalization + early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-22T16:00:21.624868Z",
     "iopub.status.busy": "2024-11-22T16:00:21.624447Z",
     "iopub.status.idle": "2024-11-22T16:01:09.765928Z",
     "shell.execute_reply": "2024-11-22T16:01:09.764835Z",
     "shell.execute_reply.started": "2024-11-22T16:00:21.624834Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6268 - loss: 0.8520 - val_accuracy: 0.8472 - val_loss: 0.4172\n",
      "Epoch 2/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.8928 - loss: 0.3015 - val_accuracy: 0.8551 - val_loss: 0.4112\n",
      "Epoch 3/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9334 - loss: 0.1844 - val_accuracy: 0.8607 - val_loss: 0.4265\n",
      "Epoch 4/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9593 - loss: 0.1222 - val_accuracy: 0.8651 - val_loss: 0.4846\n",
      "Epoch 5/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.0931 - val_accuracy: 0.8563 - val_loss: 0.5517\n",
      "Epoch 6/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.9758 - loss: 0.0720 - val_accuracy: 0.8603 - val_loss: 0.5685\n",
      "Epoch 7/50\n",
      "\u001b[1m1061/1061\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9814 - loss: 0.0577 - val_accuracy: 0.8662 - val_loss: 0.5981\n",
      "\u001b[1m266/266\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "BatchNormalization Neural Network Accuracy: 0.8550895381715363\n",
      "BatchNormalization Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.83      0.84      0.84      2292\n",
      "     Neutral       0.82      0.81      0.81      2548\n",
      "    Positive       0.89      0.90      0.89      3648\n",
      "\n",
      "    accuracy                           0.86      8488\n",
      "   macro avg       0.85      0.85      0.85      8488\n",
      "weighted avg       0.85      0.86      0.86      8488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# 定义神经网络模型：多隐藏层 + BatchNormalization + 输出层\n",
    "batchnorm_nn_model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),  # 第1隐藏层\n",
    "    BatchNormalization(),  # 批量归一化层\n",
    "\n",
    "    Dense(64, activation='relu'),  # 第2隐藏层\n",
    "    BatchNormalization(),  # 批量归一化层\n",
    "\n",
    "    Dense(32, activation='relu'),  # 第3隐藏层\n",
    "    BatchNormalization(),  # 批量归一化层\n",
    "\n",
    "    Dense(3, activation='softmax')  # 输出层\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "batchnorm_nn_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 定义 EarlyStopping 回调函数\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # 监控验证集损失\n",
    "    patience=5,                # 容忍验证集损失不下降的次数\n",
    "    restore_best_weights=True  # 恢复验证集表现最好的权重\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "batchnorm_nn_model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    validation_data=(X_test, y_test_onehot),\n",
    "    epochs=50,                # 设置较大的最大训练轮数\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]  # 添加 EarlyStopping 回调\n",
    ")\n",
    "\n",
    "# 获取预测结果\n",
    "y_pred_probs = batchnorm_nn_model.predict(X_test)  # 预测概率\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)  # 将概率转换为类别索引\n",
    "y_true = np.argmax(y_test_onehot, axis=1)  # 将 one-hot 编码转换为类别索引\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(\"BatchNormalization Neural Network Accuracy:\", accuracy)\n",
    "\n",
    "# 生成分类报告\n",
    "print(\"BatchNormalization Neural Network Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6121338,
     "sourceId": 9953305,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
